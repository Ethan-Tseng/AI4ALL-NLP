{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "Today, we're going to work on loading and cleaning the dataset. We'll write a few different functions first, and then combine them together at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this runs without output\n",
    "train_body_path = \"train_bodies.csv\"\n",
    "if not os.path.exists(train_body_path):\n",
    "    print(\"Check location for train_bodies\")\n",
    "test_body_path = \"test_bodies.csv\"\n",
    "if not os.path.exists(test_body_path):\n",
    "    print(\"Check location for test_bodies\")\n",
    "train_stance_path = \"train_stances.csv\"\n",
    "if not os.path.exists(train_stance_path):\n",
    "    print(\"Check location for train_stances\")\n",
    "test_headline_path = \"test_stances_unlabeled.csv\"\n",
    "if not os.path.exists(test_headline_path):\n",
    "    print(\"Check location for test_stances_unlabeled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning strings\n",
    "\n",
    "First, let's write a function to clean a string. This means taking in a string (word or sentence) and making sure that it is:\n",
    "- All lowercase\n",
    "- Every space is only one space long\n",
    "\n",
    "You will probably find string methods helpful for this task. Take a look at the documentation for Python strings to find some useful methods to accomplish at least the first two tasks:\n",
    "https://docs.python.org/3/library/stdtypes.html#string-methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_clean(s):\n",
    "    # TODO: lowercase a string\n",
    "    \n",
    "    # TODO: make sure all spaces are only one long\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s):\n",
    "    lowercase = s.lower()\n",
    "    return \" \".join(lowercase.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test it out!\n",
    "upper = \"ThIs SeNtenCE sHouLD bE AlL loWErCaSE\"\n",
    "clean_upper = clean(upper)\n",
    "print(clean_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces = \"this      sentence should    have only  one  space between words\"\n",
    "clean_spaces = clean(spaces)\n",
    "print(clean_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "How do we tokenize a sentence, or break it down into its component words? We can do it ourselves, but there are libraries that do a more advanced job. Let's try making our own function first using string operations. Take a peek at the documentation first:\n",
    "\n",
    "https://docs.python.org/2/library/stdtypes.html#string-methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_w_tokenize(s):\n",
    "    # TODO: write a function to split a sentence into its component words\n",
    "    # HINT: look at str.split() in the documentation. Think about where this would fail. Can you get fancier?\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_str = \"This is a sentence with words that're regular and that aren't.\"\n",
    "print(our_w_tokenize(ex_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a function using nltk to tokenize a list of words. \n",
    "\n",
    "Compare its output with the output of nltk's word tokenizer. What difference do you see? Why might we want to use a more complex tokenizer? What other kinds of words might be tricky? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to tokenize a sentence into words\n",
    "def w_tokenize(s):\n",
    "    return nltk.word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_tokenize(ex_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tokenize sentences, dividing up a paragraph into sentences. Again, we can write a bunch of rules to do this ourselves, or we can let nltk handle it. Let's try it ourselves, using the same string methods as before. Can you think of examples that might confound your function? How might you approach that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_s_tokenize(p):\n",
    "    # TODO: come up with your own simple way of splitting a paragraph into sentences. Again, look at string libraries.\n",
    "    # Think about where it might fail. Can you make it better?\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_paragraph = \"Here is a multi-sentence string (with some unusual parts). Does it handle question marks correctly? What about names like Mr. Rogers? nltk might use different rules than you do!\"\n",
    "print(our_s_tokenize(ex_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that uses the nltk method to tokenize a sentence and compare. What's different? What rules might you have forgotten? Tokenizing is a good example of where rule-based approaches are helpful, but also challenging! It's hard to anticipate every case, but sometimes it's necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to tokenize a paragraph into sentences\n",
    "def s_tokenize(p):\n",
    "    return nltk.sent_tokenize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "print(s_tokenize(ex_paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing\n",
    "Next, we're going to write a function to lemmatize our words. Lemmatizing words means converting them to their most basic form: singular (for nouns), present tense (for verbs), etc.Lemmatizing words makes it easier to compare for content, even if the words don't appear in exactly the same form. We're going to use nltk to lemmatize our words. Take a look at how it works below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Can you predict what each line will print?\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"ran\"))\n",
    "print(lemmatizer.lemmatize(\"ran\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering what the second argument (with pos= and without) is. It's an optional argument that specifies the part of speech --- without it, the lemmatizer assumes everything is a noun. You can try a few examples of your own below if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try whatever words you want here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing this ourselves would be pretty tricky, so we're going to use the nltk lemmatizer. Let's write a function that does the lemmatization on a set of word tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to take a list of word tokens and lemmatize each\n",
    "def lemmatize(word_tokens):\n",
    "    return [lemmatizer.lemmatize(t) for t in word_tokens]\n",
    "    \n",
    "print(lemmatize(w_tokenize(\"Several people running the marathon were injured.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that verbs aren't lemmatized correctly --- it's because of the optional argument. There's a way to do this with nltk's part-of-speech tagging, which is included in the challenge section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "Can you think about what kinds of words we might not care about when processing natural language for similarity?\n",
    "\n",
    "Words that occur very frequently and don't convey very much information in searches and NLP are called \"stopwords\". You can imagine some examples: the, and, a. We frequently remove these words from search queries and text comparisons to reduce some unecessary noise. Luckily, nltk has our back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Here is a list of english stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to remove stopwords from sentences.\n",
    "def remove_stopwords(word_tokens):\n",
    "    # TODO: return ONLY the words in word_tokens that DO NOT appear in stop_words\n",
    "    \n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_ex = remove_stopwords(nltk.word_tokenize(\"This sentence has meaningful words and stopwords\"))\n",
    "print(stop_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together!\n",
    "Let's put together the cleaning methods that we have to clean, tokenize, lemmatize, and remove stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs all our cleaning functions for words\n",
    "def w_super_clean(s):\n",
    "    return remove_stopwords(lemmatize(w_tokenize(clean(s))))\n",
    "\n",
    "# Performs all our cleaning functions for paragraphs\n",
    "def s_super_clean(p):\n",
    "    sentences = s_tokenize(p)\n",
    "    clean_sentences = []\n",
    "    for s in sentences:\n",
    "        clean_sentences.append(\" \".join(remove_stopwords(lemmatize(w_tokenize(clean(s))))))\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create your own \"dirty sentence\" to put your cleaning function to the test\n",
    "dirty_s = \"HeRE's a CRAzy sentence that's GOt lots of ERrors\"\n",
    "clean_w = w_super_clean(dirty_s)\n",
    "print(clean_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_p = \"HeRE's a CRAzy sentence that's GOt lots of ERrors. There iS more than ONe SenTence.\"\n",
    "clean_s = s_super_clean(dirty_p)\n",
    "print(clean_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Now, let's try to load in our data so that we can start working with headlines and articles. \n",
    "We're going to load the article bodies into a dictionary, and the headlines and stances into lists of tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function loads a body file and breaks it into words and sentences\n",
    "def load_body(filename):\n",
    "    id2body = # TODO: make an empty dict\n",
    "    id2body_sentences = # TODO: make an empty dict\n",
    "    \n",
    "    # These lines open the file and read in each row\n",
    "    with open(filename, encoding='utf-8', errors='ignore') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        data = list(reader)\n",
    "        for row in data:\n",
    "            \n",
    "            # This line gets the Body ID for this row\n",
    "            body_id = row['Body ID']\n",
    "            # This line gets the article body\n",
    "            body = str(row['articleBody'])\n",
    "            # This line strips leading and trailing spaces from the body\n",
    "            body = body.strip()\n",
    "            \n",
    "            body_words =  # TODO: clean the body words\n",
    "            \n",
    "            body_sentences = # TODO: clean the body sentences\n",
    "            \n",
    "            # TODO: Add this article body to the id2body dict using its body_id as a key\n",
    "            \n",
    "            # TODO: Add the list of lists clean_body_sentences to the id2body_sentences dict using its body_id as a key\n",
    "            \n",
    "    \n",
    "    return id2body, id2body_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a moment to run\n",
    "id2body, id2body_sentences = load_body(train_body_path)\n",
    "test_id2body, test_id2body_sentences = load_body(test_body_path)\n",
    "\n",
    "# We're going to add the test bodies to our overall body database for ease of access later\n",
    "id2body.update(test_id2body)\n",
    "id2body_sentences.update(test_id2body_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure that our data structure looks about right!\n",
    "print(len(id2body))\n",
    "print(id2body['0'])\n",
    "print(id2body_sentences['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_title(filename):\n",
    "    titles = # TODO: make an empty list\n",
    "    \n",
    "    # Open csv and read in rows\n",
    "    with open(filename, errors='ignore') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        raw_data = list(reader)\n",
    "        for row in raw_data:\n",
    "            \n",
    "            body_id = #TODO: get the body id cell\n",
    "            title = #TODO: get the headline cell \n",
    "            title = str(title).strip()\n",
    "            \n",
    "            clean_title = #TODO: clean title words\n",
    "        \n",
    "            title_id_tuple = (clean_title, body_id)\n",
    "            # TODO: append title_id_tuple to the titles list\n",
    "            \n",
    "            \n",
    "    return titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_title(test_headline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stance(filename):\n",
    "    stances = # TODO: make an empty list\n",
    "    with open(filename, errors='ignore') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        raw_data = list(reader)\n",
    "        for row in raw_data:\n",
    "            title = # TODO: get headline\n",
    "            body_id = # TODO: get body id\n",
    "            stance = # TODO: get stance\n",
    "            \n",
    "            stance = stance.strip()\n",
    "            \n",
    "            clean_title = # TODO: clean title words\n",
    "            \n",
    "            stance_tuple = (clean_title, body_id, stance)\n",
    "            # TODO: append stance_tuple to stances\n",
    "            \n",
    "    return stances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stances = load_stance(train_stance_path)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've gotten the data into the form we need so that we can work with it in the coming days. There's some challenge work related to speeding up and improving our data cleaning process below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Here are two challenge problems! You can work on whichever one interests you.\n",
    "\n",
    "### Challenge 1:\n",
    "One of the flaws of our cleaning function is that it doesn't lemmatize non-nouns correctly (because nltk requires a part of speech argument to process words as non-nouns). Fortunately, nltk provides a method for part-of-speech tagging. Can you write a new lemmatizing function that tags parts of speech first and uses those tags to do a better job lemmatizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def better_lem(word_tokens):\n",
    "    lemmas = []\n",
    "    word_tags = # TODO: use nltk's pos_tag method to do part-of-speech tagging for the sentence\n",
    "    \n",
    "    # word_tags should be a tuple of words and tags\n",
    "    # this elif structure will return a string pos that can be used as an argument to the lemmatize function\n",
    "    for word, tag in word_tags:\n",
    "        if tag.startswith('J'):\n",
    "            pos = wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            pos = wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            pos = wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            pos = wordnet.ADV\n",
    "        else:\n",
    "            pos = ''\n",
    "        # TODO: if pos is not '', add the correct part-of-speech lemma to the lemmas list\n",
    "        \n",
    "        # TODO: otherwise, add the noun version (no second argument)\n",
    "        \n",
    "    return lemmas\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"Verbs like ran run running or be are am is should come out about the same\"\n",
    "print(better_lem(test_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: \n",
    "Before, we allowed nltk to do all of our lemmatization for us. Using the documentation on the Python package re, can you use a regular expression to do basic lemmatization on nouns by making them singular? Think about what patterns usually characterize plural nouns, and replace those with the singular form. Do as many different cases as you have time for!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_lem(w):\n",
    "    # TODO: write a regular expression that replaces plural endings with singular endings. \n",
    "    # Hint: you can run the same word through a regular expression multiple times ---  \n",
    "    # if no matches are found, it won't be changed\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test it out!\n",
    "print(re_lem(\"horses\"))\n",
    "print(re_lem(\"cats\"))\n",
    "print(re_lem(\"ponies\"))\n",
    "print(re_lem(\"kitties\"))\n",
    "print(re_lem(\"cacti\"))\n",
    "print(re_lem(\"octopi\"))\n",
    "print(re_lem(\"geese\"))\n",
    "print(re_lem(\"mooses\"))\n",
    "print(re_lem(\"fish\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
