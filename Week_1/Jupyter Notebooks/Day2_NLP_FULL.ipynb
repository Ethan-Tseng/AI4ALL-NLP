{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will start working with the Fake News Challenge dataset (https://github.com/FakeNewsChallenge/fnc-1). By the end of today, we hope you will be comfortable:\n",
    "1. Importing and exporting data from a Jupyter notebook\n",
    "2. Examining the structure of the dataset (how many rows and columns are in the dataset? What does each row and column represent?)\n",
    "3. With the goal of the Fake News Challenge. How good are you at identifying misleading headlines?  Do you think you can beat an AI?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anytime you see ``______ # TODO: FILL IN HERE.`` in the code, you should replace the ``______`` with your own code.\n",
    "\n",
    "As always, ask your neighbors or an instructor if you have any questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Representation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import the packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "\n",
    "# Adjust settings so that we can fully see the dataset below\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to https://github.com/FakeNewsChallenge/fnc-1 and download the following files:\n",
    "    - train_stances.csv\n",
    "    - train_bodies.csv\n",
    "\n",
    "These files contain the data for the Fake News Challenge in the \"csv\" format. \"csv\" stands for \"comma-separate values\". We'll use this information later, when we tell the program how to load this data.\n",
    "\n",
    "PRO TIP: Make sure that the downloaded datasets and this jupyter notebook are in the same directory (folder), else you will have problems later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Understand what the data contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start looking at the data, we should understand what it contains and where it comes from.\n",
    "\n",
    "Go to http://www.fakenewschallenge.org/ and read up to, and including, the \"DATA\" section. Try to answer the following questions:\n",
    "\n",
    "    - What information do these .csv files contain?\n",
    "    - What is the classification goal posed by the Fake News Challenge?\n",
    "    - How was this data collected?\n",
    "    \n",
    "Discuss your answers with your neighbor or an instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following code to load the data you downloaded into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stances = pd.read_csv(\"train_stances.csv\", encoding = \"utf-8\")\n",
    "train_bodies = pd.____(\"train_bodies.csv\", encoding = \"utf-8\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Look at the layout of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``.columns`` parameter of a DataFrame tells us the name of the columns. Run the following cells to examine the column names of the DataFrames we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stances.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bodies.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each dataset contains the column 'Body ID'. We will use this column later in order to match the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at the columns of the dataset, let's look at the rows. How many rows are in each dataset? We can use the ``.shape`` parameter to tell us about the number of rows in each dataset. Can you guess what the second number, returned by ``.shape``, corresponds to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bodies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of rows in ``train_stances`` is different to the number of rows in ``train_bodies``.  Why is this? How many unique entries are there for the ``Body ID`` variable in each dataset? We can use the ``pd.unique`` function to figure this out, and we can use ``[]`` (square parentheses) to isolate a single column in each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(pd.unique(train_stances['Body ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(pd.unique(train_bodies['Body ID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! While the ``train_bodies`` dataset has one row for each value of the ``Body ID`` variable, there are multiple rows in ``train_stances`` corresponding to a single value of ``Body ID``.  Let's look at examples in each dataset corresponding to a value of 0 for the ``Body ID``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We expect there to be one row of the train_bodies dataset returned here:\n",
    "train_bodies[train_bodies['Body ID'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Â Let's check that there is exactly one row in train_bodies corresponding to Body ID = 0 by using the .shape parameter \n",
    "# discussed before:\n",
    "train_bodies[train_bodies['Body ID'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We expect there to be multiple rows of the train_stances dataset returned here: (Note: the .head() function \n",
    "# really useful for returning just a few rows)\n",
    "train_stances[train_stances['Body ID'] == 0].____()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use the shape parameter once more to check:\n",
    "train_stances[train_stances['Body ID'] == 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the ``train_stances`` dataset contains multiple headlines corresponding to each article in ``train_bodies``, and our task is to train an AI to identify the correct headline associated with each article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Re-organize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we loaded contains all the information we need, but it puts different pieces of information about the same article in different DataFrames. To make the data easier to work with, we'd like to put the information about each article in one DataFrame.\n",
    "\n",
    "This function reads a dataset into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_bodies, train_stances, on='Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's examine the shape of the newly created dataset:\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's also look at the first few rows of this merged dataset:\n",
    "train_data.____()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Exporting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created an amalgamated dataset, we'd like to export this, so that we can use it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_data.csv\", index=False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Challenge\n",
    "(Only if time permits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Extra challenge\" sections are a more unguided exploration into the concepts we've discussed. You'll notice less scaffolding for the code -- try implementing these concepts from scratch, and feel free to ask your neighbors or an instructor if you have any questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try looking at some other information about the data. Try to answer the following questions (and any others you think would be useful) -- feel free to look things up online along the way.\n",
    "\n",
    "    - How many examples are there for each stance? (For instance, how many \"unrelated\" examples are there?).\n",
    "        - Are each of the stances equally represented?\n",
    "    - In general, how long are the headings and article bodies?\n",
    "        - Does this differ for different stances?\n",
    "        - How much do these counts vary between different examples?\n",
    "    - When you read the article and headline and try to decide whether the headline agrees, disagrees, discusses the content of the article or is irrelevant, what factors do you consider?  How can we code these up so that a computer can gain our intuition and perform this classification by itself?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Stance Variable Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: the goal of this challenge is to teach the computer to 'read' in article/headline pairs and predict the associated value of the Stance variable (UNRELATED/DISCUSS/AGREE/DISAGREE).  First, we should look at the number of examples that we have for each value of the stance variable that we can use for teaching the computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of stance variables in the dataset:\n",
    "train_data.Stance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And as a proportion of the rows in the dataset: (use an attribute of train_data that you learned about yesterday to\n",
    "#Â divide the count values by the number of rows in the dataset)\n",
    "train_data.Stance.value_counts()/train_data.____[0] # TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have a very uneven dataset, with many more examples of UNRELATED article/headline pairs compared to DISCUSS/AGREE/DISAGREE article/headline pairs. We will discuss the consequences of this uneven distribution later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Exploring the data - Can we find features that are different across stance groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we are going to work with a reduced subset of the training dataset, which has an equal number of examples from each Stance category. We will return to working with the full dataset later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8a. Randomly sample 100 examples from each stance class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrelated = train_data[train_data['Stance'] == 'unrelated'].sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discuss = train_data[train_data['Stance'] == 'discuss'].sample(_____) # TODO: FILL IN HERE\n",
    "agree = train_data[train_data['Stance'] == 'agree']._____(_____) # TODO: FILL IN HERE\n",
    "disagree = train_data[train_data['Stance'] == _____].sample(100) # TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Check that each class has one hundred samples:\n",
    "unrelated._____[0] # TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8b. What distinguishes each class? \n",
    "When headlines and articles are 'unrelated', is the proportion of words from the article contained in the headline different to that for other stance classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the proportion of words that are contained in a headline that are also contained in an article for \n",
    "# a headline-article example.  Let's create a function that reads in an example (single row in any of the dataframes we \n",
    "#Â created above) and returns the proportion of the headline that is contained in the article:\n",
    "def find_headline_in_article_proportion(example):\n",
    "    # Separate out all of the individual words in headline:\n",
    "    headline_words = str.split(example['Headline'])\n",
    "    # Separate out all of the individual words in the article:\n",
    "    article_words = str.split(example['articleBody'])\n",
    "    # Counter to count the number of words in the headline that are contained in the article\n",
    "    counter = 0 \n",
    "    # Now iterate through each of the words in the headline and check if the word also exists in the article\n",
    "    for word in headline_words:\n",
    "        if word in article_words:\n",
    "            # If the word in the headline is contained in the article, add 1 to the counter\n",
    "            counter += 1\n",
    "    # Since the length of each headline is different, what we really want to compare across examples is the proportion \n",
    "    # of the headline that is contained in the article \n",
    "    proportion = counter/len(headline_words)\n",
    "    return proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply the function to all examples in each of the four dataframes we created above.\n",
    "# We will save the results to a list:\n",
    "proportions_unrelated = [] # Save results to this list\n",
    "for i in range(unrelated.shape[0]):\n",
    "    # Extract example\n",
    "    this_example = unrelated.iloc[i]\n",
    "    # Save result of calling find_headline_in_article_proportion() on example to proportions_unrelated list\n",
    "    proportions_unrelated.append(find_headline_in_article_proportion(this_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same for each of the other stance classes:\n",
    "# discuss class\n",
    "proportions_discuss= [] \n",
    "for i in range(discuss.shape[0]):\n",
    "    this_example = discuss.iloc[____] # TODO: FILL IN HERE\n",
    "    proportions_discuss.append(find_headline_in_article_proportion(_____)) # TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agree class\n",
    "proportions_agree= [] \n",
    "for i in range(agree.____): # TODO: FILL IN HERE\n",
    "    this_example = agree.iloc[i]\n",
    "    proportions_agree.append(find_headline_in_article_proportion(this_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disagree class\n",
    "proportions_______= [] # TODO: FILL IN HERE\n",
    "for i in range(disagree.shape[0]):\n",
    "    this_example = disagree.iloc[i]\n",
    "    proportions_disagree.append(_______(this_example)) # TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8c. Calculate mean proportion for each stance class.\n",
    "What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unrelated\n",
    "np.mean(proportions_unrelated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agree\n",
    "np.mean(________agree) # TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discuss\n",
    "_____.mean(proportions_discuss) # TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disagree\n",
    "np._____(proportions_disagree) # TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8d. Visualize the differences in proportions across stance classes\n",
    "We will create a boxplot to show the differences in proportions across stance classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_plot = [proportions_unrelated, proportions_agree, proportions_disagree, ___________] # TODO: FILL IN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure instance\n",
    "fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "# Create an axes instance\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Create the boxplot\n",
    "bp = ax.boxplot(data_to_plot)\n",
    "# Set the xaxis tick marks\n",
    "ax.set_xticklabels(['Unrelated', 'Agree', 'Disagree', 'Discuss'])\n",
    "# Set the yaxis label\n",
    "ax.set_ylabel('Proportion headline in article')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice from the boxplot? From the boxplot, could you come up with a rule for the computer to use in order to classify article-headline examples as UNRELATED/AGREE/DISAGREE/DISCUSS? What about if the challenge was only to classify pairs as either UNRELATED or RELATED (with RELATED = AGREE + DISAGREE + DISCUSS)? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
