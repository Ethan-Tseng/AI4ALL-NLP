{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF on Fake News Challenge\n",
    "\n",
    "Let's get started. First, we have to load the training data. What packages do we need for that? From here on out, replace ___ with the necessary code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul  2 16:50:39 2019\n",
    "@author: Nobline\n",
    "\"\"\"\n",
    "\n",
    "#import pandas\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's actually load the training data. What pandas command might you need? Take a look back at previous slides if you need a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data\n",
    "bodies = ____________ #read train_bodies.csv\n",
    "headlines = ____________  #read train_stances.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, we have our data, we can start cleaning our data and extracting our TFIDF features. Remember what we did yesterday? This cell will take quite some time to run. In the meantime, take some time to look at the documentation of the package we will be using today: sklearn's TfidfVectorizer\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):  #doc is a column in a DataFrame\n",
    "    #lowercasing\n",
    "    for ___ in ______:  #iterate through the dataframe\n",
    "        doc.set_value(i, _______)  #lowercase all the words\n",
    "    print(\"LOWERCASE\")\n",
    "    print(doc)\n",
    "    \n",
    "    #remove punctuation\n",
    "    for ___ in _____:  #iterate through the Dataframe\n",
    "        doc.set_value(i, ______)  #remove punctuation\n",
    "    print(\"REMOVE PUNCT\")\n",
    "    print(doc)\n",
    "\n",
    "    #remove stopwords\n",
    "    for ___ in ____:  #iterate through the dataframe\n",
    "        doc.set_value(i, _____)  #remove stopwords\n",
    "    print(\"REMOVE STOPWORDS\")\n",
    "    print(doc)\n",
    "    \n",
    "    #tokenize\n",
    "    for _____ in _____:  #iterate through the dataframe\n",
    "        doc.set_value(i, ______) #tokenize the document in the row\n",
    "    print(\"TOKENIZE\")\n",
    "    print(doc)\n",
    "    \n",
    "    #lemmatize\n",
    "    for i in _______:   #iterate through the dataframe\n",
    "        for j in ______:  #iterate through each item in the list of tokenized words\n",
    "            doc.iloc[i][j] = lemmatizer.lemmatize(_____)  #access the word at row i, index j of tokenized list\n",
    "    print(\"LEMMATIZE\")\n",
    "    print(doc)\n",
    "     \n",
    "clean(______)  #clean the bodies\n",
    "clean(_______)  #clean the headlines\n",
    "train_data = pd.______(bodies, headlines, on='Body ID')   #merge the bodies and headlines into one DataFrame\n",
    "train_data  #print out the find preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start to think about how we're going to extract our TFIDF vectors. What package(s) do we need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract tfidf vectors of article body and headline\n",
    "from ______ import _______  # import TfidfVectorizer --> use the documentation for guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a function called **tfidf_extractor** that takes two parameters: b (set of article bodies), h (set of headlines). This function will extract the TFIDF vectors and print out the resulting matrices. Take a look at the comments and fill in the appropriate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace TfidfVectorizer's default tokenizer and preprocessor\n",
    "def default(doc):\n",
    "    return doc\n",
    "\n",
    "#extract tfidf scores for each word in the vocabulary for each document\n",
    "def tfidf_extractor(b, h):\n",
    "    #initialize TfidfVectorizer with the following parameters:\n",
    "    #analyzer = \"word\"\n",
    "    #preprocessor = default\n",
    "    #tokenizer = default\n",
    "    #token_pattern = None\n",
    "    vect = _________\n",
    "\n",
    "    bodyTFIDF = vect._______(b)     #fit and transform to the article bodies (extract the tfidf scores for the bodies)\n",
    "    \n",
    "    #print results\n",
    "    print(\"BODY VOCABULARY\")\n",
    "    print(vect.vocabulary_)\n",
    "    print(\"BODIES\")\n",
    "    print(bodyTFIDF)\n",
    "        \n",
    "    headlineTFIDF = vect._______   #fit and transform to the headlines (extract the tfidf scores for the headlines)\n",
    "    \n",
    "    #print results\n",
    "    print(\"HEADLINE VOCABULARY\")\n",
    "    print(vect.vocabulary_)\n",
    "    print(\"HEADLINES\")\n",
    "    print(headlineTFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAST STEP! We're almost done! Let's call the function we just made on our set of articles bodies and headlines and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main action!\n",
    "tfidf_extractor(train_data['articleBody'], train_data['Headline'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
