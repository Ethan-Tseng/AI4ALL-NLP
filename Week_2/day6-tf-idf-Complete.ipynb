{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction & TF-IDF\n",
    "\n",
    "Today, we're going to implement our tf-idf counter and sketch out the broad outlines of our feature extraction code. Keep in mind, we want everything we write to be compatible with the cleaning and loading code we wrote yesterday, since that's the data that we'll be extracting features from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding TF: Dictionary counting\n",
    "\n",
    "For our TF function, we're going to want to count the number of times a word occurs in a document. Then, we must divide by the total length of the document to find the TF value of the word for that document.\n",
    "\n",
    "For now, we will have each document be a list of individual words, rather than one long sentence. This makes it easier to work with. We'll learn more about formatting data tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - find the number of times keyword shows up in document\n",
    "# - find the length of the document\n",
    "# - output the TF value\n",
    "def find_tf(keyword, document):\n",
    "    keyword_count = document.count(keyword)\n",
    "    return keyword_count / len(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF scaling\n",
    "\n",
    "What we're going to do now is write a function that finds the relative frequency of any word across all documents (that is, what portion of documents contain that word). We will later use this term to scale individual term counts for each text document.\n",
    "\n",
    "We're going to structure this function to read from a dictionary of text bodies. The keys in the dictionary are IDs, while the values are the documents, which are long Strings. Tomorrow, during data cleaning, this is the format we will use to represent the Fake News Challenge data.\n",
    "\n",
    "Here is the documentation for the dictionary type. We're going to want a function that lets us loop through the keys and items in a dictionary --- can you find it? \n",
    "\n",
    "https://docs.python.org/3/tutorial/datastructures.html#dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the idf for a particular keyword for a corpus of documents, which is a dictionary of ids and documents\n",
    "def find_idf(keyword, corpus):\n",
    "    docs_containing = 0\n",
    "    idf = {}\n",
    "    \n",
    "    # TODO: loop through the items in id2body using a dictionary method\n",
    "    for (doc_id, doc) in corpus.items():\n",
    "        if keyword in doc:\n",
    "            docs_containing += 1\n",
    "    \n",
    "    total_docs = len(corpus)\n",
    "    \n",
    "    if docs_containing == 0:\n",
    "        return float('inf') # this shouldn't matter since the IDF doesn't mean anything if no documents contain the word\n",
    "    else:\n",
    "        return log(total_docs / docs_containing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Week_2/article2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-efc7666776eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mexample_article_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"article1.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Week_2/article2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Week_2/article3.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Week_2/article4.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Week_2/article5.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Week_2/article6.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Week_2/article7.txt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mexample_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_article_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-efc7666776eb>\u001b[0m in \u001b[0;36mget_corpus\u001b[0;34m(article_names)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0marticle_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticle_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marticle_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenized_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-efc7666776eb>\u001b[0m in \u001b[0;36mget_tokenized_article\u001b[0;34m(article_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_tokenized_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mraw_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtokenized_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Week_2/article2.txt'"
     ]
    }
   ],
   "source": [
    "# Let's test your IDF function! Here is some example data (7 news articles about Brazil) which\n",
    "# we have prepared for you. You're welcome to read over this code, but you don't need to do \n",
    "# anything with it for now: just run it to get the example data.\n",
    "# We will learn more about preparing and cleaning data tomorrow.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_tokenized_article(article_name):\n",
    "    file = open(article_name, \"r\")\n",
    "    raw_file = file.read()\n",
    "    tokenized_file = word_tokenize(raw_file)\n",
    "    return tokenized_file\n",
    "\n",
    "def get_corpus(article_names):\n",
    "    corpus = {}\n",
    "    for article_name in article_names:\n",
    "        corpus[article_name] = get_tokenized_article(article_name)\n",
    "    return corpus\n",
    "\n",
    "example_article_names = [\"Week_2/article1.txt\", \"Week_2/article2.txt\", \"Week_2/article3.txt\", \"Week_2/article4.txt\", \"Week_2/article5.txt\", \"Week_2/article6.txt\", \"Week_2/article7.txt\"]\n",
    "example_corpus = get_corpus(example_article_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at some of the idf values. Do these look about right to you?\n",
    "print(find_idf(\"the\", example_corpus))\n",
    "print(find_idf(\"a\", example_corpus))\n",
    "print(find_idf(\"an\", example_corpus))\n",
    "print(find_idf(\"we\", example_corpus))\n",
    "print(find_idf(\"Brazil\", example_corpus))\n",
    "print(find_idf(\"animal\", example_corpus))\n",
    "print(find_idf(\"of\", example_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now, we've written functions that can calculate TF and IDF values for any word in a corpus of documents. Let's put it together to write a TF-IDF function that finds the TF-IDF values for a word in a corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(keyword, corpus):\n",
    "    idf = find_idf(keyword, corpus)\n",
    "    tf_values = {}\n",
    "    tf_idf_values = {}\n",
    "    for (doc_id, doc) in corpus.items():\n",
    "        tf_values[doc_id] = find_tf(keyword, doc)\n",
    "        tf_idf_values[doc_id] = tf_values[doc_id] * idf\n",
    "    \n",
    "    return tf_idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out your function below! Do your results make sense?\n",
    "tf_idf(\"Amazon\", example_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Search function\n",
    "\n",
    "If you have extra time, try using our TF-IDF calculations to return the most relevant document from a corpus, based on a list of keywords!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - get the TF-IDF value of each keyword for each document\n",
    "# - sum the TF-IDF values to find the total TF-IDF value of those keywords for that document\n",
    "# - return the ID of the document with the highest value\n",
    "def get_most_relevant(keywords, corpus):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
