{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction & TF-IDF\n",
    "\n",
    "Today, we're going to implement our tf-idf counter and sketch out the broad outlines of our feature extraction code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding TF: Dictionary counting\n",
    "\n",
    "For our TF function, we're going to want to count the number of times a word occurs in a document. Then, we must divide by the total length of the document to find the TF value of the word for that document.\n",
    "\n",
    "For now, we will have each document be a list of individual words, rather than one long sentence. This makes it easier to work with. We'll learn more about formatting data tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - find the number of times keyword shows up in document (document is a list of words) -> [\"The\", \"Prime\", \"Minister\", ...]\n",
    "# - find the length of the document\n",
    "# - output the TF value\n",
    "def find_tf(keyword, document):\n",
    "    keyword_count = _______\n",
    "    return ________/________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF scaling\n",
    "\n",
    "What we're going to do now is write a function that finds the relative frequency of any word across all documents (that is, what portion of documents contain that word). We will later use this term to scale individual term counts for each text document.\n",
    "\n",
    "We're going to structure this function to read from a dictionary of text bodies. The keys in the dictionary are IDs, while the values are the documents, which are long Strings. Tomorrow, during data cleaning, this is the format we will use to represent the Fake News Challenge data.\n",
    "\n",
    "Here is the documentation for the dictionary type. We're going to want a function that lets us loop through the keys and items in a dictionary --- can you find it? \n",
    "\n",
    "https://docs.python.org/3/tutorial/datastructures.html#dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the idf for a particular keyword for a corpus of documents, which is a dictionary of ids and documents\n",
    "def find_idf(keyword, corpus):\n",
    "    docs_containing = 0\n",
    "    idf = {}\n",
    "    \n",
    "    # TODO: loop through the items in id2body using a dictionary method\n",
    "    for (doc_id, doc) in corpus.items():\n",
    "        if _________\n",
    "            docs_containing += 1\n",
    "    \n",
    "    total_docs = ______ # total number of documents\n",
    "    \n",
    "    if docs_containing == 0:\n",
    "        return float('inf') # this shouldn't matter since the IDF doesn't mean anything if no documents contain the word\n",
    "    else:\n",
    "        return #the idf --> log (total # of document / # of documents containing the keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test your IDF function! Here is some example data (7 news articles about Brazil) which\n",
    "# we have prepared for you. You're welcome to read over this code, but you don't need to do \n",
    "# anything with it for now: just run it to get the example data.\n",
    "# We will learn more about preparing and cleaning data tomorrow.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_tokenized_article(article_name):\n",
    "    file = open(article_name, \"r\")\n",
    "    raw_file = file.read()\n",
    "    tokenized_file = word_tokenize(raw_file)\n",
    "    return tokenized_file\n",
    "\n",
    "def get_corpus(article_names):\n",
    "    corpus = {}\n",
    "    for article_name in article_names:\n",
    "        corpus[article_name] = get_tokenized_article(article_name)\n",
    "    return corpus\n",
    "\n",
    "example_article_names = [\"article1.txt\", \"article2.txt\", \"article3.txt\", \"article4.txt\", \"article5.txt\", \"article6.txt\", \"article7.txt\"]\n",
    "example_corpus = get_corpus(example_article_names)\n",
    "print(example_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at some of the tf and idf values. Do these look about right to you?\n",
    "print(\"TF\")\n",
    "print(find_tf(\"the\", _______)) #find the tf of \"the\" in the first article\n",
    "print(find_tf(\"a\", _______)) #copy what you did in the previous line\n",
    "print(find_tf(\"an\", _______))\n",
    "print(find_tf(\"we\", _______))\n",
    "print(find_tf(\"Brazil\", _______))\n",
    "print(find_tf(\"animal\", _______))\n",
    "print(find_tf(\"of\", _______))\n",
    "\n",
    "print(\"\\nIDF\")\n",
    "print(find_idf(\"the\", ______))\n",
    "print(find_idf(\"a\", ______))\n",
    "print(find_idf(\"an\", ______))\n",
    "print(find_idf(\"we\", ______))\n",
    "print(find_idf(\"Brazil\", ______))\n",
    "print(find_idf(\"animal\", ______))\n",
    "print(find_idf(\"of\", ______))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now, we've written functions that can calculate TF and IDF values for any word in a corpus of documents. Let's put it together to write a TF-IDF function that finds the TF-IDF values for a word in a corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(keyword, corpus):\n",
    "    idf = _____ #find the idf of the keyword in corpus\n",
    "    tf_values = {}\n",
    "    tf_idf_values = {}\n",
    "    for ____ in ____: #traverse through the corpus dictionary\n",
    "        tf_values[doc_id] = _____ # find the tf score of the keyword in the doc\n",
    "        tf_idf_values[doc_id] = ____  #multiple the tf score with the idf\n",
    "    \n",
    "    return tf_idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test out your function below! Do your results make sense?\n",
    "tf_idf(\"Amazon\", example_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Get Back to the Fake News Challenge\n",
    "Now let's relate this back to the Fake News Challenge. How can solving a search query help us determine what's fake and what's real? How can TFIDF be a metric that represents each document and headline? Brainstom amonst yourselves. (Think about a collection of TFIDF scores for each unique word in a document).\n",
    "\n",
    "To finish off the day, we'll be creating a matrix of TFIDF scores. Each row is the list of TFIDF scores for a word in the vocabulary of the corpus. Each column corresponds to a separate document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_corpus = ____ #what is a kind of collection in python that only adds unique elements\n",
    "for _____ in ____: #traverse through the example_corpus\n",
    "    vocabulary_corpus = vocabulary_corpus.union(set(example_corpus[____]))\n",
    "#print(vocabulary_corpus)\n",
    "tfidf_matrix = [[]]\n",
    "r = 0 #row index counter\n",
    "for ____ in ____: #traverse through vocabulary_corpus\n",
    "    tfidf = tf_idf(____, example_corpus)\n",
    "    for (doc_id, score) in tfidf.items():\n",
    "        tfidf_matrix[r].append(______)\n",
    "        tfidf_matrix.append([])\n",
    "    r += 1\n",
    "r = 0\n",
    "for val in vocabulary_corpus:\n",
    "    print(val, end = ' ')\n",
    "    print(tfidf_matrix[r])\n",
    "    r += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Search function\n",
    "\n",
    "If you have extra time, try using our TF-IDF calculations to return the most relevant document from a corpus, based on a list of keywords!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - get the TF-IDF value of each keyword for each document\n",
    "# - sum the TF-IDF values to find the total TF-IDF value of those keywords for that document\n",
    "# - return the ID of the document with the highest value\n",
    "def get_most_relevant(keywords, corpus):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
